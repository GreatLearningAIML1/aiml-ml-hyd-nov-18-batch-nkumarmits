{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R8_Internal_Lab_Questions-HYD_AIML_Nov18.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NFfDTfhlaEI_"
      },
      "source": [
        "# Transfer Learning MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rNwbqCFRaEJC"
      },
      "source": [
        "* Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n",
        "* Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IXrn3heBaEJa"
      },
      "source": [
        "### Import MNIST data and create 2 datasets with one dataset having digits from 0 to 4 and other from 5 to 9 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "poVVTgTMndNA"
      },
      "source": [
        "#### Import the mnist dataset from keras datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pjDuiK6ztgOK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2314afe5-20f4-4fbd-d9e1-d9213ceceda7"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQxBz3FNhfo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KMOJ66rpnh6C"
      },
      "source": [
        "#### Creating two datasets one with digits below 5 and one with 5 and above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xa33p7rdniFx",
        "colab": {}
      },
      "source": [
        "#datasets below 5 digits\n",
        "x_train_b5 = x_train[y_train < 5]\n",
        "y_train_b5 = y_train[y_train < 5]\n",
        "x_test_b5 = x_test[y_test < 5]\n",
        "y_test_b5 = y_test[y_test < 5]\n",
        "\n",
        "#datasets above 5 digits\n",
        "x_train_a5 = x_train[y_train >= 5]\n",
        "y_train_a5 = y_train[y_train >= 5] - 5  # make classes start at 0 for\n",
        "x_test_a5 = x_test[y_test >= 5]         # np_utils.to_categorical\n",
        "y_test_a5 = y_test[y_test >= 5] - 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9qU14lYL9A5g"
      },
      "source": [
        "### Check \n",
        "\n",
        "Verify shapes of x_train, y_train, x_test and y_test for both the datasets with the below given shapes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9cx1YQbCor2r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3b45243b-9844-474c-f0fe-a5016d7af93e"
      },
      "source": [
        "print(f'Shape of x_train is {x_train.shape}.')\n",
        "print(f'Shape of y_train is {y_train.shape}.')\n",
        "print(f'Shape of x_test is {x_test.shape}.')\n",
        "print(f'Shape of y_test is {y_test.shape}.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_train is (60000, 28, 28).\n",
            "Shape of y_train is (60000,).\n",
            "Shape of x_test is (10000, 28, 28).\n",
            "Shape of y_test is (10000,).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z9OrszhJ0SgJ",
        "outputId": "acc5c485-35b3-48b6-89f4-2980ee60f5fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(x_train_b5.shape)\n",
        "print(y_train_b5.shape)\n",
        "print(x_test_b5.shape)\n",
        "print(y_test_b5.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30596, 28, 28)\n",
            "(30596,)\n",
            "(5139, 28, 28)\n",
            "(5139,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sJswV4xk9jQS",
        "outputId": "4b79e6b3-8631-4853-90f2-f986d6e65a28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(x_train_a5.shape)\n",
        "print(y_train_a5.shape)\n",
        "print(x_test_a5.shape)\n",
        "print(y_test_a5.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(29404, 28, 28)\n",
            "(29404,)\n",
            "(4861, 28, 28)\n",
            "(4861,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cB9BPFzr9oDF"
      },
      "source": [
        "### Let us take only the dataset (x_train, y_train, x_test, y_test) for Integers 0 to 4 in MNIST\n",
        "### Reshape x_train and x_test to a 4 Dimensional array (channel = 1) to pass it into a Conv2D layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DavgN2I7hfpN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "463bfc58-052a-4ae9-8a4c-1daf2bbe31f0"
      },
      "source": [
        "from keras import backend as K\n",
        "print(K.image_data_format())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "channels_last\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FlQRPfFzaEJx",
        "colab": {}
      },
      "source": [
        "x_train_b5 = x_train_b5.reshape((30596, 28, 28, 1))\n",
        "x_test_b5 = x_test_b5.reshape((5139, 28, 28, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jLQr-b3F-hw8"
      },
      "source": [
        "### Change into float32 datatype and Normalize x_train and x_test by dividing it by 255.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PlEZIAG5-g2I",
        "colab": {}
      },
      "source": [
        "x_train_b5 = x_train_b5.astype('float32')\n",
        "x_test_b5 = x_test_b5.astype('float32')\n",
        "x_train_b5 /= 255\n",
        "x_test_b5 /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZSQrJFHgocNA"
      },
      "source": [
        "### Check\n",
        "\n",
        "Verify the shapes of the X_train and X_test with the shapes given below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "utZ2zrpDoej9",
        "outputId": "5c58c2ac-de61-4a88-cdd0-2dccf5123427",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('X_train shape:', x_train_b5.shape)\n",
        "print('X_test shape:', x_test_b5.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (30596, 28, 28, 1)\n",
            "X_test shape: (5139, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pytVBaw4-vMi"
      },
      "source": [
        "### Use One-hot encoding to divide y_train and y_test into required no of output classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V48xiua4-uUi",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoItEX_Ohfpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = len(set(y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4_l1NZkhfpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_b5 = to_categorical(y_train_b5, num_classes=classes)\n",
        "y_test_b5 = to_categorical(y_test_b5, num_classes=classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "elPkI44g_C2b"
      },
      "source": [
        "### Build a sequential model with 2 Convolutional layers with 32 kernels of size (3,3) followed by a Max pooling layer of size (2,2) followed by a drop out layer to be trained for classification of digits 0-4  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nOeyulnRp1Hp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "dd7f38f6-a766-401a-b89e-0f6a56e0bcf9"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0707 08:34:30.201012 140088000448384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0707 08:34:30.218308 140088000448384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0707 08:34:30.221576 140088000448384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0707 08:34:30.242835 140088000448384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0707 08:34:30.246217 140088000448384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0707 08:34:30.259054 140088000448384 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sJQaycRO_3Au"
      },
      "source": [
        "### Post that flatten the data and add 2 Dense layers with 128 neurons and neurons = output classes with activation = 'relu' and 'softmax' respectively. Add dropout layer inbetween if necessary  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qivtHVYVjAe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOZeRbK7t9AT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3a44c5d1-44db-4071-cb02-8c3f93a6d6e3"
      },
      "source": [
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(units=classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0707 08:34:30.396915 140088000448384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0707 08:34:30.425610 140088000448384 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "my1P09bxAv8H"
      },
      "source": [
        "### Print the training and test accuracy for 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yf7F8Gdutbf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "859c7d32-053d-4b8f-810b-312096f15271"
      },
      "source": [
        "model.fit(x_train_b5, y_train_b5, batch_size=32, epochs=5, validation_data=(x_test_b5, y_test_b5))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0707 08:34:30.562882 140088000448384 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 30596 samples, validate on 5139 samples\n",
            "Epoch 1/5\n",
            "30596/30596 [==============================] - 9s 304us/step - loss: 0.1534 - acc: 0.9500 - val_loss: 0.0260 - val_acc: 0.9930\n",
            "Epoch 2/5\n",
            "30596/30596 [==============================] - 7s 243us/step - loss: 0.0467 - acc: 0.9853 - val_loss: 0.0105 - val_acc: 0.9967\n",
            "Epoch 3/5\n",
            "30596/30596 [==============================] - 7s 243us/step - loss: 0.0352 - acc: 0.9891 - val_loss: 0.0101 - val_acc: 0.9961\n",
            "Epoch 4/5\n",
            "30596/30596 [==============================] - 7s 244us/step - loss: 0.0271 - acc: 0.9919 - val_loss: 0.0113 - val_acc: 0.9961\n",
            "Epoch 5/5\n",
            "30596/30596 [==============================] - 8s 250us/step - loss: 0.0267 - acc: 0.9924 - val_loss: 0.0064 - val_acc: 0.9975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f687e22b080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "musZS-o1hfps",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "814a5853-151d-44fa-95d1-e72d19905348"
      },
      "source": [
        "sc = model.evaluate(x_test_b5, y_test_b5, verbose=0)\n",
        "print(f'Test Score {sc[0]}, Test Accuracy {sc[1]}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Score 0.006447878525380851, Test Accuracy 0.9974703249659467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEomDPuvhfpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('model_b5.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z78o3WIjaEJ3"
      },
      "source": [
        "### Use the model trained on 0 to 4 digit classification and train it on the dataset which has digits 5 to 9  (Using Transfer learning keeping only the dense layers to be trainable)\n",
        "\n",
        "### Make only the dense layers to be trainable and convolutional layers to be non-trainable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DqRYoYlXCw_L"
      },
      "source": [
        "#### Check model summary to see model layer names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMwf8QmHhfpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "brN7VZHFaEJ4",
        "colab": {}
      },
      "source": [
        "below4Model = load_model('model_b5.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k24fHhlUslH6",
        "outputId": "4d115c4b-0a0a-465b-cd0e-1ee766cf46e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "for layers in below4Model.layers:\n",
        "    print(below4Model.name)\n",
        "    if('dense' not in layers.name):\n",
        "        layers.trainable = False\n",
        "        print(layers.name + ' is not trainable\\n')\n",
        "    if('dense' in layers.name):\n",
        "        print(layers.name + ' is trainable\\n')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sequential_1\n",
            "conv2d_1 is not trainable\n",
            "\n",
            "sequential_1\n",
            "max_pooling2d_1 is not trainable\n",
            "\n",
            "sequential_1\n",
            "dropout_1 is not trainable\n",
            "\n",
            "sequential_1\n",
            "conv2d_2 is not trainable\n",
            "\n",
            "sequential_1\n",
            "max_pooling2d_2 is not trainable\n",
            "\n",
            "sequential_1\n",
            "dropout_2 is not trainable\n",
            "\n",
            "sequential_1\n",
            "flatten_1 is not trainable\n",
            "\n",
            "sequential_1\n",
            "dense_1 is trainable\n",
            "\n",
            "sequential_1\n",
            "dropout_3 is not trainable\n",
            "\n",
            "sequential_1\n",
            "dense_2 is trainable\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8908wI0itQZ7"
      },
      "source": [
        "### Do the required preprocessing for `x_train_a5` also same as `x_train_b5` and for `y_train_a5` same as `y_train_b5`\n",
        "\n",
        "1. Reshape\n",
        "2. Change to float32 datatype\n",
        "3. Normalize (dividing with 255)\n",
        "4. y_train and y_test Convert into one-hot vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YUr9Haghfp6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a6521ff-fd0f-4e86-e30c-3bea13bc1f33"
      },
      "source": [
        "x_train_a5.shape, x_test_a5.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((29404, 28, 28), (4861, 28, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lCFcYHTm6-cE",
        "colab": {}
      },
      "source": [
        "x_train_a5 = x_train_a5.reshape((29404, 28, 28, 1))\n",
        "x_test_a5 = x_test_a5.reshape((4861, 28, 28, 1))\n",
        "x_train_a5 = x_train_a5.astype('float32')\n",
        "x_test_a5 = x_test_a5.astype('float32')\n",
        "x_train_a5 /= 255\n",
        "x_test_a5 /= 255\n",
        "\n",
        "y_train_a5 = to_categorical(y_train_a5, num_classes=classes)\n",
        "y_test_a5 = to_categorical(y_test_a5, num_classes=classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1SYDxFsvuAVE"
      },
      "source": [
        "### Check\n",
        "\n",
        "Verify the shapes with the given below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5AANrUvVt_6U",
        "outputId": "3a7a83d9-5b91-4181-a036-4172433c7b7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(x_train_a5.shape)\n",
        "print(y_train_a5.shape)\n",
        "print(x_test_a5.shape)\n",
        "print(y_test_a5.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(29404, 28, 28, 1)\n",
            "(29404, 10)\n",
            "(4861, 28, 28, 1)\n",
            "(4861, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SoDozqghCJZ4"
      },
      "source": [
        "## Print the accuracy for classification of digits 5 to 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT8cMLIshfqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "below4Model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiaJUkORhfqH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "50235a6d-9484-4d5a-aec5-71779f5da857"
      },
      "source": [
        "below4Model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               102528    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 113,386\n",
            "Trainable params: 103,818\n",
            "Non-trainable params: 9,568\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9fCxgb5s49Cj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "3b4aa90f-8cd9-4183-b34e-7e90c2def792"
      },
      "source": [
        "below4Model.fit(x_train_a5, y_train_a5, batch_size=32, epochs=5, validation_data=(x_test_a5, y_test_a5))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 29404 samples, validate on 4861 samples\n",
            "Epoch 1/5\n",
            "29404/29404 [==============================] - 6s 191us/step - loss: 0.3109 - acc: 0.9101 - val_loss: 0.0515 - val_acc: 0.9844\n",
            "Epoch 2/5\n",
            "29404/29404 [==============================] - 5s 177us/step - loss: 0.0952 - acc: 0.9702 - val_loss: 0.0365 - val_acc: 0.9881\n",
            "Epoch 3/5\n",
            "29404/29404 [==============================] - 5s 177us/step - loss: 0.0746 - acc: 0.9780 - val_loss: 0.0294 - val_acc: 0.9897\n",
            "Epoch 4/5\n",
            "29404/29404 [==============================] - 5s 177us/step - loss: 0.0685 - acc: 0.9777 - val_loss: 0.0277 - val_acc: 0.9916\n",
            "Epoch 5/5\n",
            "29404/29404 [==============================] - 5s 177us/step - loss: 0.0569 - acc: 0.9820 - val_loss: 0.0234 - val_acc: 0.9924\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f687d3ae9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LRWizZIpCUKg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4f89376-c260-4ee1-9caa-e63ebd40b227"
      },
      "source": [
        "sc = below4Model.evaluate(x_test_a5, y_test_a5, verbose=0)\n",
        "print(f'Test Score {sc[0]}, Test Accuracy {sc[1]}')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Score 0.023357077801853095, Test Accuracy 0.9923883974490846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0zDuRecXzEtr"
      },
      "source": [
        "# Text classification using TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xMPlEJhHzb6P"
      },
      "source": [
        "###  Load the dataset from sklearn.datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fe-B59u3zHNb",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRrMemVQzbHU",
        "colab": {}
      },
      "source": [
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-sZX0UbJzmg5"
      },
      "source": [
        "### Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CITr_5aXziJ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "9accda07-0207-4f73-bd15-e7c71477f6a2"
      },
      "source": [
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "I0707 08:35:40.253712 140088000448384 twenty_newsgroups.py:247] Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
            "I0707 08:35:40.257681 140088000448384 twenty_newsgroups.py:80] Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xcESc5QXzr6p"
      },
      "source": [
        "### Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ysInblUMzpvl",
        "colab": {}
      },
      "source": [
        "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LKLT0U-hfqZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e250220c-3aea-4a5d-9670-ce07167d10ef"
      },
      "source": [
        "twenty_test.data[0:2]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"From: brian@ucsd.edu (Brian Kantor)\\nSubject: Re: HELP for Kidney Stones ..............\\nOrganization: The Avant-Garde of the Now, Ltd.\\nLines: 12\\nNNTP-Posting-Host: ucsd.edu\\n\\nAs I recall from my bout with kidney stones, there isn't any\\nmedication that can do anything about them except relieve the pain.\\n\\nEither they pass, or they have to be broken up with sound, or they have\\nto be extracted surgically.\\n\\nWhen I was in, the X-ray tech happened to mention that she'd had kidney\\nstones and children, and the childbirth hurt less.\\n\\nDemerol worked, although I nearly got arrested on my way home when I barfed\\nall over the police car parked just outside the ER.\\n\\t- Brian\\n\",\n",
              " 'From: rind@enterprise.bih.harvard.edu (David Rind)\\nSubject: Re: Candida(yeast) Bloom, Fact or Fiction\\nOrganization: Beth Israel Hospital, Harvard Medical School, Boston Mass., USA\\nLines: 37\\nNNTP-Posting-Host: enterprise.bih.harvard.edu\\n\\nIn article <1993Apr26.103242.1@vms.ocom.okstate.edu>\\n banschbach@vms.ocom.okstate.edu writes:\\n>are in a different class.  The big question seems to be is it reasonable to \\n>use them in patients with GI distress or sinus problems that *could* be due \\n>to candida blooms following the use of broad-spectrum antibiotics?\\n\\nI guess I\\'m still not clear on what the term \"candida bloom\" means,\\nbut certainly it is well known that thrush (superficial candidal\\ninfections on mucous membranes) can occur after antibiotic use.\\nThis has nothing to do with systemic yeast syndrome, the \"quack\"\\ndiagnosis that has been being discussed.\\n\\n\\n>found in the sinus mucus membranes than is candida.  Women have been known \\n>for a very long time to suffer from candida blooms in the vagina and a \\n>women is lucky to find a physician who is willing to treat the cause and \\n>not give give her advise to use the OTC anti-fungal creams.\\n\\nLucky how?  Since a recent article (randomized controlled trial) of\\noral yogurt on reducing vaginal candidiasis, I\\'ve mentioned to a \\nnumber of patients with frequent vaginal yeast infections that they\\ncould try eating 6 ounces of yogurt daily.  It turns out most would\\nrather just use anti-fungal creams when they get yeast infections.\\n\\n>yogurt dangerous).  If this were a standard part of medical practice, as \\n>Gordon R. says it is, then the incidence of GI distress and vaginal yeast \\n>infections should decline.\\n\\nAgain, this just isn\\'t what the systemic yeast syndrome is about, and\\nhas nothing to do with the quack therapies that were being discussed.\\nThere is some evidence that attempts to reinoculate the GI tract with\\nbacteria after antibiotic therapy don\\'t seem to be very helpful in\\nreducing diarrhea, but I don\\'t think anyone would view this as a\\nquack therapy.\\n-- \\nDavid Rind\\nrind@enterprise.bih.harvard.edu\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DriL2yZ50DQq"
      },
      "source": [
        "###  a.  You can access the values for the target variable using .target attribute \n",
        "###  b. You can access the name of the class in the target variable with .target_names\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vlUuai99z1hX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad145b97-d0fb-4c5e-c3d1-c2ed9dd0f7c0"
      },
      "source": [
        "twenty_train.target"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 3, ..., 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VEKzaDfSz5E-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c320e1ae-ec84-4787-b272-9b6b7d8b78c8"
      },
      "source": [
        "twenty_train.target_names"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "clBMKHzC0_N1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "14df151a-78b4-401c-8f5b-935983939276"
      },
      "source": [
        "twenty_train.data[0:2]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['From: sd345@city.ac.uk (Michael Collier)\\nSubject: Converting images to HP LaserJet III?\\nNntp-Posting-Host: hampton\\nOrganization: The City University\\nLines: 14\\n\\nDoes anyone know of a good way (standard PC application/PD utility) to\\nconvert tif/img/tga files into LaserJet III format.  We would also like to\\ndo the same, converting to HPGL (HP plotter) files.\\n\\nPlease email any response.\\n\\nIs this the correct group?\\n\\nThanks in advance.  Michael.\\n-- \\nMichael Collier (Programmer)                 The Computer Unit,\\nEmail: M.P.Collier@uk.ac.city                The City University,\\nTel: 071 477-8000 x3769                      London,\\nFax: 071 477-8565                            EC1V 0HB.\\n',\n",
              " \"From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\nSubject: help: Splitting a trimming region along a mesh \\nOrganization: University Of Kentucky, Dept. of Math Sciences\\nLines: 28\\n\\n\\n\\n\\tHi,\\n\\n\\tI have a problem, I hope some of the 'gurus' can help me solve.\\n\\n\\tBackground of the problem:\\n\\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \\n\\tmapping of a 3d Bezier patch into 2d. The area in this domain\\n\\twhich is inside a trimming loop had to be rendered. The trimming\\n\\tloop is a set of 2d Bezier curve segments.\\n\\tFor the sake of notation: the mesh is made up of cells.\\n\\n\\tMy problem is this :\\n\\tThe trimming area has to be split up into individual smaller\\n\\tcells bounded by the trimming curve segments. If a cell\\n\\tis wholly inside the area...then it is output as a whole ,\\n\\telse it is trivially rejected. \\n\\n\\tDoes any body know how thiss can be done, or is there any algo. \\n\\tsomewhere for doing this.\\n\\n\\tAny help would be appreciated.\\n\\n\\tThanks, \\n\\tAni.\\n-- \\nTo get irritated is human, to stay cool, divine.\\n\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hTz4EaN_1WGc"
      },
      "source": [
        "### Now with dependent and independent data available for both train and test datasets, using TfidfVectorizer fit and transform the training data and test data and get the tfidf features for both"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgjRl99Whfqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNkqe-FBhfqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = twenty_train.data\n",
        "X_test = twenty_test.data\n",
        "y_train = twenty_train.target\n",
        "y_test = twenty_test.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H5G477f81C0Z",
        "colab": {}
      },
      "source": [
        "# TfidfVectorizer\n",
        "vect = TfidfVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw8SAUeehfq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_tfidf = vect.fit_transform(X_train).todense()\n",
        "y_train_tfidf = y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDZgofXKhfq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(X_train_tfidf, columns=vect.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUtbNFHchfq5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "16b8e826-881e-4376-fb43-951f7a00f0c8"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>00</th>\n",
              "      <th>000</th>\n",
              "      <th>0000</th>\n",
              "      <th>0000001200</th>\n",
              "      <th>000005102000</th>\n",
              "      <th>0001</th>\n",
              "      <th>000100255pixel</th>\n",
              "      <th>00014</th>\n",
              "      <th>000406</th>\n",
              "      <th>0007</th>\n",
              "      <th>000usd</th>\n",
              "      <th>0010</th>\n",
              "      <th>001004</th>\n",
              "      <th>0010580b</th>\n",
              "      <th>001125</th>\n",
              "      <th>001200201pixel</th>\n",
              "      <th>0014</th>\n",
              "      <th>001642</th>\n",
              "      <th>00196</th>\n",
              "      <th>002</th>\n",
              "      <th>0028</th>\n",
              "      <th>003258u19250</th>\n",
              "      <th>0033</th>\n",
              "      <th>0038</th>\n",
              "      <th>0039</th>\n",
              "      <th>004021809</th>\n",
              "      <th>004158</th>\n",
              "      <th>004627</th>\n",
              "      <th>0049</th>\n",
              "      <th>00500</th>\n",
              "      <th>005148</th>\n",
              "      <th>00630</th>\n",
              "      <th>008561</th>\n",
              "      <th>0094</th>\n",
              "      <th>00am</th>\n",
              "      <th>00index</th>\n",
              "      <th>00pm</th>\n",
              "      <th>01</th>\n",
              "      <th>0100</th>\n",
              "      <th>010116</th>\n",
              "      <th>...</th>\n",
              "      <th>zoerasterism</th>\n",
              "      <th>zola</th>\n",
              "      <th>zolf</th>\n",
              "      <th>zolft</th>\n",
              "      <th>zoloft</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zonal</th>\n",
              "      <th>zone</th>\n",
              "      <th>zonker</th>\n",
              "      <th>zoo</th>\n",
              "      <th>zooid</th>\n",
              "      <th>zool</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zooming</th>\n",
              "      <th>zooms</th>\n",
              "      <th>zopfi</th>\n",
              "      <th>zorasterism</th>\n",
              "      <th>zorg</th>\n",
              "      <th>zorn</th>\n",
              "      <th>zrz</th>\n",
              "      <th>zsoft</th>\n",
              "      <th>zt</th>\n",
              "      <th>zubkoff</th>\n",
              "      <th>zues</th>\n",
              "      <th>zug</th>\n",
              "      <th>zumder</th>\n",
              "      <th>zur</th>\n",
              "      <th>zurich</th>\n",
              "      <th>zurlo</th>\n",
              "      <th>zus</th>\n",
              "      <th>zvi</th>\n",
              "      <th>zvonko</th>\n",
              "      <th>zwart</th>\n",
              "      <th>zyeh</th>\n",
              "      <th>zyklon</th>\n",
              "      <th>zyxel</th>\n",
              "      <th>zz</th>\n",
              "      <th>zzz</th>\n",
              "      <th>ªl</th>\n",
              "      <th>íålittin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.29521</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.055283</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.057185</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 35788 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         00  000  0000  0000001200  ...   zz  zzz   ªl  íålittin\n",
              "0  0.000000  0.0   0.0         0.0  ...  0.0  0.0  0.0       0.0\n",
              "1  0.000000  0.0   0.0         0.0  ...  0.0  0.0  0.0       0.0\n",
              "2  0.000000  0.0   0.0         0.0  ...  0.0  0.0  0.0       0.0\n",
              "3  0.000000  0.0   0.0         0.0  ...  0.0  0.0  0.0       0.0\n",
              "4  0.055283  0.0   0.0         0.0  ...  0.0  0.0  0.0       0.0\n",
              "\n",
              "[5 rows x 35788 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tp_fDINJ1t4L"
      },
      "source": [
        "### Use logisticRegression with tfidf features as input and targets as output and train the model and report the train and test accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gJag1fOhfq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "THlN2b5d1yQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "9e451957-4bb3-40d2-9570-978e13bfc36e"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "logreg.fit(df, y_train_tfidf)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uLZ0ASBhfrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test_tfidf = vect.transform(X_test)\n",
        "y_test_tfidf = y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_SRM7-2hfrD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2927a79b-15e0-4157-c739-65fedf33dc39"
      },
      "source": [
        "y_pred_class = logreg.predict(X_test_tfidf)\n",
        "y_pred_class"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, ..., 2, 2, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B30gbISihfrG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "610f05da-494e-40ac-8a01-5b3875c2f24c"
      },
      "source": [
        "print(metrics.accuracy_score(y_test_tfidf, y_pred_class))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8868175765645806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEdtFn12hfrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}